from dataset import load_data
import torch
from parse import get_parse
from utils import fix_seed
import os

from transformers import RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling
from transformers import TrainingArguments, Trainer
from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling
from transformers import TrainingArguments, Trainer

from peft import TaskType, LoraConfig, get_peft_model

class LoraModel(RobertaForMaskedLM):
    def prepare_inputs_for_generation(self, input_ids, **kwargs):
        attention_mask = kwargs.get("attention_mask", None)
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }



def main(args):
    fix_seed()

    trainset = load_data(args)  # 加载数据
    
    tokenizer = RobertaTokenizer.from_pretrained(os.path.join("model", args.model),
                                                 max_length=args.max_length)
    
    def prepare_dataset(dataset, tokenizer, block_size=512):
        # 数据预处理    
        def tokenize_function(examples):
            return tokenizer(examples["text"], 
                             padding="max_length", 
                             truncation=True, 
                             max_length=block_size)
            
        dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"], num_proc=10)
        dataset.set_format("torch", columns=["input_ids", "attention_mask"])
        return dataset
    
    trainset = prepare_dataset(trainset, tokenizer)

    # 加载预训练模型
    model = LoraModel.from_pretrained(os.path.join("model", args.model))

    # 配置训练策略
    training_args = TrainingArguments(
        output_dir=os.path.join('result', args.model),
        do_train=True,
        num_train_epochs=2,
        per_device_train_batch_size=6,
        gradient_accumulation_steps=4,
        save_strategy="epoch",
    )

    # 数据加载
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=0.15,
    )
    
    # LoRA参数配置
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=32
    )
    lora_model = get_peft_model(model, peft_config)
    
    # 打印lora微调的参数
    print(lora_model.print_trainable_parameters())
    
    # 训练
    trainer = Trainer(
        model=lora_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=trainset,
    )
    trainer.train()
    
    # 模型保存
    lora_model.save_pretrained(os.path.join('model', args.model+'-lora'))


if __name__ == '__main__':
    args = get_parse()
    main(args)
